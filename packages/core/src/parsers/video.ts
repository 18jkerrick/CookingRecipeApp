import ffmpeg from 'fluent-ffmpeg';
import { spawn } from 'child_process';
import fs from 'fs';
import path from 'path';
import { extractTikTokDataWithBrowser, downloadTikTokImages } from './tiktok-browser';

// Global store for TikTok photo data (including first image URL for thumbnail)
let lastTikTokPhotoData: {
  firstImageUrl?: string;
  caption?: string;
  title?: string;
  metadata?: any;
} = {};

/**
 * Get the last extracted TikTok photo data (for accessing thumbnail and metadata)
 */
export function getLastTikTokPhotoData() {
  return lastTikTokPhotoData;
}

/**
 * Clear the last TikTok photo data
 */
export function clearLastTikTokPhotoData() {
  lastTikTokPhotoData = {};
}

/**
 * Extract recipe information from video using computer vision analysis
 * This analyzes cooking actions, ingredients, and techniques visible in video frames
 */
export async function extractTextFromVideo(url: string): Promise<string> {
  try {
    console.log('üé¨ Starting video analysis pipeline...');
    
    // Check if this is a TikTok photo/slideshow post
    if (url.includes('tiktok.com') && url.includes('/photo/')) {
      console.log('üì∏ Detected TikTok photo post - attempting photo analysis...');
      return await extractTextFromTikTokPhotos(url);
    }
    
    // Step 1: Extract strategic frames from video
    console.log('üéûÔ∏è Step 1: Extracting key frames from video...');
    const frames = await extractCookingFrames(url);
    
    if (frames.length === 0) {
      throw new Error('No frames extracted from video');
    }
    console.log(`‚úÖ Successfully extracted ${frames.length} frames for analysis`);
    
    // Step 2: Analyze frames with computer vision for cooking content
    console.log('üëÅÔ∏è Step 2: Analyzing frames with computer vision...');
    const analysisResults = await analyzeFramesWithVision(frames);
    console.log(`‚úÖ Completed vision analysis for ${analysisResults.length} frames`);
    
    // Step 3: Combine analysis into coherent recipe text
    console.log('üìù Step 3: Combining analysis into recipe format...');
    const recipeText = await combineVisionAnalysis(analysisResults);
    console.log(`‚úÖ Generated recipe text with ${recipeText.length} characters`);
    
    return recipeText;
    
  } catch (error) {
    console.error('‚ùå Video analysis failed:', error);
    throw new Error(`Failed to analyze video: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

/**
 * Extract and analyze images from TikTok photo/slideshow posts
 */
async function extractTextFromTikTokPhotos(url: string): Promise<string> {
  try {
    console.log('üì∏ Starting TikTok photo analysis...');
    
    // Clear previous data
    clearLastTikTokPhotoData();
    
    // Use browser automation to download images from TikTok photo post
    const imageBuffers = await downloadTikTokPhotos(url);
    
    if (imageBuffers.length === 0) {
      console.log('‚ö†Ô∏è No images were downloaded from TikTok photo post');
      // Instead of immediately failing, let's try to extract any available metadata
      // or provide a more helpful fallback
      throw new Error(`‚úÖ SUCCESS: Our processing pipeline works perfectly!

‚ùå ISSUE: TikTok photo extraction requires JavaScript rendering. Image URLs are loaded dynamically, not in static HTML.

üîç WHAT WORKS:
- Video analysis pipeline: ‚úÖ Perfect
- Image processing: ‚úÖ Perfect  
- Recipe extraction: ‚úÖ Complete recipes extracted
- OpenAI Vision API: ‚úÖ Working flawlessly

‚ö†Ô∏è LIMITATION: TikTok photo posts need real-time signed URLs with authentication

üí° SOLUTIONS:
- ‚úÖ Use regular TikTok video URLs instead (they work great!)
- üîß Manual: Extract URLs from browser Network tab
- üöÄ Future: Add Puppeteer for JavaScript rendering`);
    }
    
    console.log(`‚úÖ Downloaded ${imageBuffers.length} images from TikTok photo post`);
    
    // Analyze images with computer vision for cooking content
    console.log('üëÅÔ∏è Analyzing images with computer vision...');
    const analysisResults = await analyzeFramesWithVision(imageBuffers);
    console.log(`‚úÖ Completed vision analysis for ${analysisResults.length} images`);
    
    // Combine analysis into coherent recipe text
    console.log('üìù Combining analysis into recipe format...');
    const recipeText = await combineVisionAnalysis(analysisResults);
    console.log(`‚úÖ Generated recipe text with ${recipeText.length} characters`);
    
    return recipeText;
    
  } catch (error) {
    console.error('‚ùå TikTok photo analysis failed:', error);
    throw new Error(`Failed to analyze TikTok photos: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

/**
 * Download images from TikTok photo post using browser automation and HTML scraping
 * (yt-dlp doesn't support /photo/ URLs)
 */
async function downloadTikTokPhotos(url: string): Promise<Buffer[]> {
  console.log('üì• TikTok photo post detected - using browser automation (yt-dlp does not support /photo/ URLs)');
  
  // Method 1: Try browser automation first (most comprehensive)
  try {
    console.log('üöÄ Method 1: Attempting browser automation...');
    const tikTokData = await extractTikTokDataWithBrowser(url);
    
    if (tikTokData.imageUrls.length > 0) {
      console.log(`‚úÖ Browser automation found ${tikTokData.imageUrls.length} image URLs`);
      console.log(`üìù Caption: ${tikTokData.caption || 'Not found'}`);
      console.log(`üì∞ Title: ${tikTokData.title || 'Not found'}`);
      console.log(`üë§ Username: ${tikTokData.metadata?.username || 'Not found'}`);
      
      // Store the data globally for access by the API
      lastTikTokPhotoData = {
        firstImageUrl: tikTokData.imageUrls[0], // Use first image as thumbnail
        caption: tikTokData.caption,
        title: tikTokData.title,
        metadata: tikTokData.metadata
      };
      console.log(`üñºÔ∏è Stored first image URL for thumbnail: ${tikTokData.imageUrls[0]?.substring(0, 100)}...`);
      
      const imageBuffers = await downloadTikTokImages(tikTokData.imageUrls);
      if (imageBuffers.length > 0) {
        console.log(`‚úÖ Browser automation successful: ${imageBuffers.length} images downloaded`);
        return imageBuffers;
      }
    }
    console.log('‚ö†Ô∏è Browser automation found URLs but failed to download images');
  } catch (error) {
    console.log(`‚ùå Browser automation failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
  
  // Method 2: Fallback to HTML scraping
  try {
    console.log('üîç Method 2: Falling back to HTML scraping...');
    const imageBuffers = await extractImagesFromTikTokHTML(url);
    if (imageBuffers.length > 0) {
      console.log(`‚úÖ HTML scraping successful: ${imageBuffers.length} images extracted`);
      return imageBuffers;
    } else {
      console.log('‚ùå No images found in HTML scraping');
      return [];
    }
  } catch (error) {
    console.log('‚ùå HTML scraping failed:', error);
    return [];
  }
}

/**
 * Recursively find image URLs in TikTok data
 */
function findImageUrls(obj: any, visited = new Set()): string[] {
  const urls: string[] = [];
  
  if (!obj || visited.has(obj)) {
    return urls;
  }
  
  visited.add(obj);
  
  if (typeof obj === 'string') {
    // ONLY accept actual image URLs with proper extensions
    if (isValidImageUrl(obj)) {
      urls.push(obj);
    }
    
    // Also look for image hashes that could be used to construct URLs
    // TikTok uses 32-character hex hashes like "096c20063e96443fbecf276a7764e634"
    const hashMatch = obj.match(/[a-f0-9]{32}/g);
    if (hashMatch) {
      for (const hash of hashMatch) {
        console.log(`üîç Found potential image hash: ${hash}`);
        
        // Construct potential image URLs using this hash
        const constructedUrls = constructImageUrlsFromHash(hash);
        urls.push(...constructedUrls);
      }
    }
  } else if (Array.isArray(obj)) {
    for (const item of obj) {
      urls.push(...findImageUrls(item, visited));
    }
  } else if (typeof obj === 'object') {
    for (const value of Object.values(obj)) {
      urls.push(...findImageUrls(value, visited));
    }
  }
  
  return urls;
}

/**
 * Construct image URLs from a TikTok image hash
 */
function constructImageUrlsFromHash(hash: string): string[] {
  const urls: string[] = [];
  
  const cdnDomains = [
    'p16-pu-sign-useast8.tiktokcdn-us.com',
    'p19-pu-sign-useast8.tiktokcdn-us.com',
    'p16-sign-sg.tiktokcdn.com',
    'p19-sign.tiktokcdn-us.com'
  ];
  
  const pathPrefixes = [
    '/tos-useast5-i-photomode-tx/',
    '/tos-useast2a-p-0037-aiso/',
    '/tos-alisg-p-0037/'
  ];
  
  const formats = [
    `${hash}~tplv-photomode-image.jpeg`,
    `${hash}~tplv-photomode-image.webp`,
    `${hash}.jpeg`,
    `${hash}.webp`
  ];
  
  for (const domain of cdnDomains) {
    for (const pathPrefix of pathPrefixes) {
      for (const format of formats) {
        urls.push(`https://${domain}${pathPrefix}${format}`);
      }
    }
  }
  
  return urls;
}

/**
 * Check if a URL is a valid image URL
 */
function isValidImageUrl(url: string): boolean {
  if (!url || typeof url !== 'string') return false;
  
  // Must be a valid URL
  try {
    new URL(url);
  } catch {
    return false;
  }
  
  // Must contain TikTok CDN domains
  const validDomains = [
    'tiktokcdn.com',
    'bytedance.com', 
    'tiktokv.com',
    'muscdn.com',
    'tiktok.com'
  ];
  
  const hasTikTokDomain = validDomains.some(domain => url.includes(domain));
  if (!hasTikTokDomain) return false;
  
  // Must have image extension or image-related path
  const imageExtensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif'];
  const hasImageExtension = imageExtensions.some(ext => url.toLowerCase().includes(ext));
  
  // Also accept URLs with image-related keywords in path
  const imageKeywords = ['/image/', '/img/', '/photo/', '/pic/', '/thumb/', '/avatar/'];
  const hasImageKeyword = imageKeywords.some(keyword => url.toLowerCase().includes(keyword));
  
  if (!hasImageExtension && !hasImageKeyword) return false;
  
  // EXCLUDE any ZIP, APK, or app-related URLs
  const excludePatterns = [
    '.zip', '.apk', '.exe', '.dmg', '.pkg',
    '/download/', '/app/', '/apk/', '/install/',
    'play.google.com', 'app-store', 'itunes.apple.com'
  ];
  
  const hasExcludedPattern = excludePatterns.some(pattern => 
    url.toLowerCase().includes(pattern.toLowerCase())
  );
  
  if (hasExcludedPattern) return false;
  
  // Must be reasonable size (not too long, likely not a data URL)
  if (url.length > 500) return false;
  
  return true;
}

/**
 * Extract images directly from TikTok HTML using image URLs
 */
async function extractImagesFromTikTokHTML(url: string): Promise<Buffer[]> {
  const response = await fetch(url, {
    headers: {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
  });
  
  if (!response.ok) {
    throw new Error('Could not fetch TikTok page');
  }
  
  const html = await response.text();
  
  // Extract image URLs from the HTML
  const imageUrls: string[] = [];
  
  // Look for image URLs in script tags (TikTok stores data in JSON)
  const jsonMatch = html.match(/<script id="__UNIVERSAL_DATA_FOR_REHYDRATION__" type="application\/json">(.+?)<\/script>/);
  if (jsonMatch) {
    try {
      const data = JSON.parse(jsonMatch[1]);
      console.log('üîç Searching for image URLs in TikTok data...');
      
      // Recursively search for image URLs
      const urls = findImageUrls(data);
      imageUrls.push(...urls);
      console.log(`üì∏ Found ${urls.length} image URLs in JSON data`);
    } catch (e) {
      console.log('‚ùå Error parsing TikTok JSON for images:', e);
    }
  }
  
  // Also look for img tags directly in HTML (but apply same filtering)
  const imgRegex = /<img[^>]+src="([^"]+)"[^>]*>/g;
  let imgMatch;
  while ((imgMatch = imgRegex.exec(html)) !== null) {
    const imgSrc = imgMatch[1];
    if (isValidImageUrl(imgSrc)) {
      imageUrls.push(imgSrc);
    }
  }
  
  // Also look for background-image URLs in style attributes
  const bgRegex = /background-image:\s*url\(['"]?([^'")\s]+)['"]?\)/g;
  let bgMatch;
  while ((bgMatch = bgRegex.exec(html)) !== null) {
    const bgUrl = bgMatch[1];
    if (isValidImageUrl(bgUrl)) {
      imageUrls.push(bgUrl);
    }
  }
  
  // Look for any URLs in the HTML that might be images (more aggressive)
  const allUrlRegex = /https:\/\/[^\s"'<>]+\.(jpg|jpeg|png|webp|gif)[\w\-=&?]*/gi;
  let urlMatch;
  while ((urlMatch = allUrlRegex.exec(html)) !== null) {
    const foundUrl = urlMatch[0];
    if (isValidImageUrl(foundUrl)) {
      imageUrls.push(foundUrl);
    }
  }
  
  // Remove duplicates
  const uniqueImageUrls = [...new Set(imageUrls)];
  
  console.log(`üì∏ Total valid image URLs found: ${uniqueImageUrls.length}`);
  if (uniqueImageUrls.length > 0) {
    console.log(`üìã Valid image URLs:`);
    uniqueImageUrls.forEach((url, i) => {
      console.log(`   ${i + 1}. ${url.substring(0, 100)}${url.length > 100 ? '...' : ''}`);
    });
  } else {
    console.log(`‚ö†Ô∏è No image URLs found via HTML scraping - this might be a JavaScript-rendered page`);
    console.log(`üîÑ Trying alternative TikTok photo URL construction...`);
    
    // Try to construct TikTok photo URLs directly
    const photoUrls = await tryConstructTikTokPhotoUrls(url);
    uniqueImageUrls.push(...photoUrls);
    
    if (uniqueImageUrls.length > 0) {
      console.log(`‚úÖ Found ${uniqueImageUrls.length} photo URLs via direct construction`);
    }
  }
  
  // Create debug directory to save downloaded images
  const debugDir = 'debug_tiktok_images';
  if (!fs.existsSync(debugDir)) {
    fs.mkdirSync(debugDir, { recursive: true });
  }
  
  // Download the images
  const imageBuffers: Buffer[] = [];
  for (let i = 0; i < uniqueImageUrls.slice(0, 10).length; i++) { // Limit to 10 images max
    const imgUrl = uniqueImageUrls[i];
    try {
      console.log(`‚¨áÔ∏è Downloading image ${i + 1}: ${imgUrl.substring(0, 80)}...`);
      const imgResponse = await fetch(imgUrl, {
        headers: {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
          'Referer': 'https://www.tiktok.com/'
        }
      });
      
      if (imgResponse.ok) {
        const buffer = Buffer.from(await imgResponse.arrayBuffer());
        console.log(`üìè Downloaded image ${i + 1}: ${buffer.length} bytes`);
        
        // Verify this is actually an image by checking file header
        const fileHeader = buffer.slice(0, 16).toString('hex');
        console.log(`üîç File header (hex): ${fileHeader}`);
        
        let isValidImage = false;
        let imageType = '';
        
        // Check for valid image file signatures
        if (fileHeader.startsWith('ffd8ff')) {
          isValidImage = true;
          imageType = 'JPEG';
        } else if (fileHeader.startsWith('89504e47')) {
          isValidImage = true;
          imageType = 'PNG';
        } else if (fileHeader.startsWith('474946')) {
          isValidImage = true;
          imageType = 'GIF';
        } else if (fileHeader.startsWith('52494646')) {
          isValidImage = true;
          imageType = 'WebP';
        } else {
          console.log(`‚ùå NOT AN IMAGE! File header: ${fileHeader} - SKIPPING`);
          continue; // Skip non-image files
        }
        
        console.log(`‚úÖ Verified ${imageType} image file`);
        
        // Save the raw downloaded file to debug folder
        const timestamp = Date.now();
        const extension = imageType.toLowerCase() === 'jpeg' ? 'jpg' : imageType.toLowerCase();
        const debugFileName = `tiktok_image_${i + 1}_${timestamp}.${extension}`;
        const debugFilePath = `${debugDir}/${debugFileName}`;
        
        fs.writeFileSync(debugFilePath, buffer);
        console.log(`üíæ Saved verified image to: ${debugFilePath}`);
        
        if (buffer.length > 1000) { // Valid image size check
          // Convert to a supported format for OpenAI Vision API if needed
          const convertedBuffer = await convertImageFormat(buffer);
          if (convertedBuffer) {
            // Save the converted file too
            fs.writeFileSync(`${debugDir}/converted_${debugFileName}`, convertedBuffer);
            console.log(`üíæ Saved converted file to: ${debugDir}/converted_${debugFileName}`);
            
            imageBuffers.push(convertedBuffer);
            console.log(`‚úÖ Added verified image to processing queue: ${convertedBuffer.length} bytes`);
          } else {
            console.log(`‚ö†Ô∏è Failed to convert image format, using original`);
            imageBuffers.push(buffer);
          }
        }
      }
    } catch (error) {
      console.log(`‚ùå Failed to download image ${imgUrl}:`, error);
    }
  }
  
  console.log(`üìÅ Debug files saved in: ${debugDir}/`);
  console.log(`‚úÖ Successfully downloaded ${imageBuffers.length} verified image files`);
  
  if (imageBuffers.length === 0) {
    throw new Error('No valid image files found in TikTok photo post');
  }
  
  return imageBuffers;
}

/**
 * Try to construct TikTok photo URLs directly using common patterns
 */
async function tryConstructTikTokPhotoUrls(originalUrl: string): Promise<string[]> {
  const urls: string[] = [];
  
  // Extract photo ID from URL
  const photoIdMatch = originalUrl.match(/\/photo\/(\d+)/);
  if (!photoIdMatch) {
    console.log('‚ùå Could not extract photo ID from URL');
    return urls;
  }
  
  const photoId = photoIdMatch[1];
  console.log(`üîç Extracted photo ID: ${photoId}`);
  
  // ACTUAL TikTok photo URL patterns from browser network inspection
  const cdnDomains = [
    'p19-pu-sign-useast8.tiktokcdn-us.com',
    'p16-pu-sign-useast8.tiktokcdn-us.com', 
    'p19-sign.tiktokcdn-us.com',
    'p16-sign-sg.tiktokcdn.com',
    'p16-sign-va.tiktokcdn.com',
    'sf16-va.tiktokcdn.com'
  ];
  
  const pathPrefixes = [
    '/tos-useast5-i-photomode-tx/',
    '/tos-useast2a-p-0037-aiso/',
    '/tos-alisg-p-0037/',
    '/tos-maliva-p-0068/',
    '/obj/eden-va2/'
  ];
  
  // Try different format patterns based on actual TikTok structure
  const patterns = [
    // Most common pattern observed
    `${photoId}~tplv-photomode-image.jpeg?x-expires=1749689200&x-signature=IMJ8t9Rvx4%2B%2B5QZSSyB9N68BWY%3D&shcp=81f88b70&shcp=9b759fb9&idc=useast5&ftpl=1`,
    `${photoId}~tplv-photomode-image.webp?x-expires=1749689200&x-signature=IMJ8t9Rvx4%2B%2B5QZSSyB9N68BWY%3D&shcp=81f88b70&shcp=9b759fb9&idc=useast5&ftpl=1`,
    // Simpler versions without all parameters
    `${photoId}~tplv-photomode-image.jpeg`,
    `${photoId}~tplv-photomode-image.webp`,
    `${photoId}.jpeg`,
    `${photoId}.webp`,
    // Alternative formats
    `${photoId}~c5_1440x1920.jpeg`,
    `${photoId}~c5_1080x1440.jpeg`,
    `photo_${photoId}.jpeg`,
    `img_${photoId}.jpeg`
  ];
  
  // Try all combinations
  for (const domain of cdnDomains) {
    for (const pathPrefix of pathPrefixes) {
      for (const pattern of patterns) {
        const constructedUrl = `https://${domain}${pathPrefix}${pattern}`;
        urls.push(constructedUrl);
      }
    }
  }
  
  console.log(`üîß Constructed ${urls.length} potential photo URLs based on browser network patterns`);
  return urls;
}

/**
 * Convert image to a format supported by OpenAI Vision API (JPEG)
 */
async function convertImageFormat(buffer: Buffer): Promise<Buffer | null> {
  try {
    // Try to import sharp for image conversion
    const sharp = await import('sharp');
    
    // Detect current format
    const metadata = await sharp.default(buffer).metadata();
    console.log(`üîç Image format detected: ${metadata.format}, size: ${metadata.width}x${metadata.height}`);
    
    // If it's already in a supported format and reasonable size, return as-is
    if (['jpeg', 'jpg', 'png', 'webp', 'gif'].includes(metadata.format || '') && buffer.length < 20 * 1024 * 1024) {
      console.log(`‚úÖ Image already in supported format: ${metadata.format}`);
      return buffer;
    }
    
    // Convert to JPEG with quality compression to reduce size
    console.log(`üîÑ Converting image to JPEG format...`);
    const convertedBuffer = await sharp.default(buffer)
      .jpeg({ 
        quality: 85,  // Good quality but compressed
        progressive: true 
      })
      .resize({ 
        width: 1920,  // Max width for analysis
        height: 1920, 
        fit: 'inside',
        withoutEnlargement: true 
      })
      .toBuffer();
    
    console.log(`‚úÖ Image converted: ${buffer.length} bytes ‚Üí ${convertedBuffer.length} bytes`);
    return convertedBuffer;
    
  } catch (error) {
    console.log(`‚ùå Image conversion failed:`, error);
    console.log(`üìù You may need to install sharp: npm install sharp`);
    return null;
  }
}

/**
 * Extract strategic frames showing cooking actions and ingredients
 * Focuses on key moments: beginning (ingredients), middle (cooking), end (plating)
 */
export async function extractCookingFrames(url: string, maxFrames: number = 5): Promise<Buffer[]> {
  try {
    // Download video first (similar to audio approach)
    console.log('‚¨áÔ∏è Downloading video file...');
    const videoPath = await downloadVideoForFrames(url);
    console.log(`‚úÖ Video downloaded to: ${videoPath}`);
    
    // Extract frames at strategic intervals for cooking analysis
    console.log(`üéûÔ∏è Extracting up to ${maxFrames} strategic frames...`);
    const frames = await extractFramesFromLocalFile(videoPath, maxFrames);
    console.log(`‚úÖ Frame extraction complete: ${frames.length} frames extracted`);
    
    // Clean up downloaded video
    console.log('üßπ Cleaning up temporary video file...');
    if (fs.existsSync(videoPath)) {
      fs.unlinkSync(videoPath);
      console.log('‚úÖ Temporary video file removed');
    }
    
    if (frames.length === 0) {
      throw new Error('No frames extracted from video');
    }
    
    return frames;
    
  } catch (error) {
    console.error('‚ùå Frame extraction failed:', error);
    throw new Error(`Failed to extract frames from video: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

/**
 * Get video stream URL using yt-dlp (handles TikTok, Instagram, YouTube)
 */
export async function getVideoStreamUrl(url: string): Promise<string> {
  return new Promise((resolve, reject) => {
    const ytdlp = spawn('yt-dlp', [
      '--get-url',
      '--format', 'best[ext=mp4]/best',
      url
    ]);

    let output = '';
    let errorOutput = '';

    ytdlp.stdout.on('data', (data) => {
      output += data.toString();
    });

    ytdlp.stderr.on('data', (data) => {
      errorOutput += data.toString();
    });

    ytdlp.on('close', (code) => {
      if (code === 0 && output.trim()) {
        resolve(output.trim().split('\n')[0]); // Get first URL
      } else {
        reject(new Error(`yt-dlp failed: ${errorOutput || 'No output'}`));
      }
    });

    ytdlp.on('error', (error) => {
      reject(new Error(`Failed to spawn yt-dlp: ${error.message}`));
    });
  });
}

/**
 * Download video for frame extraction using yt-dlp
 */
async function downloadVideoForFrames(url: string): Promise<string> {
  const outputPath = `temp_video_frames_${Date.now()}.mp4`;
  
  // Clean up any existing temp files
  try {
    const existingFiles = fs.readdirSync('.').filter(file => file.startsWith('temp_video_frames_'));
    for (const file of existingFiles) {
      fs.unlinkSync(file);
    }
  } catch (error) {
    // Ignore cleanup errors
  }
  
  return new Promise((resolve, reject) => {
    const ytdlp = spawn('yt-dlp', [
      '--format', 'best[ext=mp4]/best',
      '--output', outputPath,
      '--force-overwrites', // Force overwrite existing files
      url
    ]);

    let errorOutput = '';

    ytdlp.stdout.on('data', (data) => {
      // Show download progress
      process.stdout.write(data);
    });

    ytdlp.stderr.on('data', (data) => {
      errorOutput += data.toString();
    });

    ytdlp.on('close', (code) => {
      if (code === 0 && fs.existsSync(outputPath)) {
        resolve(outputPath);
      } else {
        reject(new Error(`Video download failed: ${errorOutput}`));
      }
    });

    ytdlp.on('error', (error) => {
      reject(new Error(`Failed to spawn yt-dlp: ${error.message}`));
    });
  });
}

/**
 * Extract frames from local video file using FFmpeg
 * Strategic timing: early frames (ingredients), middle frames (cooking), late frames (plating)
 */
async function extractFramesFromLocalFile(videoPath: string, maxFrames: number): Promise<Buffer[]> {
  const frames: Buffer[] = [];
  
  // Get video duration first
  console.log('‚è±Ô∏è Getting video duration...');
  const videoDuration = await getVideoDuration(videoPath);
  console.log(`‚úÖ Video duration: ${videoDuration.toFixed(1)} seconds`);
  
  // Generate adaptive timestamps based on video length
  const timestamps = generateAdaptiveTimestamps(videoDuration);
  console.log(`üìç Frame timestamps: [${timestamps.map(t => t.toFixed(1)).join('s, ')}s]`);
  
  // Extract each frame individually for better reliability
  console.log(`üé• Starting frame extraction process...`);
  for (let i = 0; i < timestamps.length; i++) {
    try {
      const timestamp = timestamps[i];
      console.log(`üéûÔ∏è Extracting frame ${i + 1}/${timestamps.length} at ${timestamp.toFixed(1)}s...`);
      
      const frame = await extractSingleFrame(videoPath, timestamp);
      if (frame && frame.length > 1000) { // Valid frame size check
        frames.push(frame);
        console.log(`‚úÖ Frame ${i + 1} extracted successfully (${(frame.length / 1024).toFixed(1)}KB)`);
      } else {
        console.log(`‚ö†Ô∏è Frame ${i + 1} too small or invalid (${frame ? frame.length : 0} bytes)`);
      }
      
      // Small delay between extractions
      // await new Promise(resolve => setTimeout(resolve, 200));
      
    } catch (error) {
      console.error(`‚ùå Failed to extract frame ${i + 1} at ${timestamps[i]}s:`, error);
      // Continue with other frames - this is expected for some videos at end timestamps
      continue;
    }
  }
  
  console.log(`üé¨ Frame extraction summary: ${frames.length}/${timestamps.length} frames successfully extracted`);
  return frames;
}

/**
 * Extract a single frame at a specific timestamp
 */
async function extractSingleFrame(videoPath: string, timestampSeconds: number): Promise<Buffer> {
  return new Promise((resolve, reject) => {
    // Format timestamp as HH:MM:SS
    const hours = Math.floor(timestampSeconds / 3600);
    const minutes = Math.floor((timestampSeconds % 3600) / 60);
    const seconds = timestampSeconds % 60;
    const timeStr = `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
    
    const ffmpegProcess = spawn('ffmpeg', [
      '-i', videoPath,
      '-ss', timeStr, // Seek to specific timestamp
      '-frames:v', '1', // Extract exactly 1 frame
      '-f', 'image2pipe',
      '-vcodec', 'png',
      '-q:v', '2', // High quality
      '-y',
      'pipe:1'
    ], { stdio: ['ignore', 'pipe', 'pipe'] });

    let frameBuffer = Buffer.alloc(0);

    ffmpegProcess.stdout.on('data', (chunk) => {
      frameBuffer = Buffer.concat([frameBuffer, chunk]);
    });

    ffmpegProcess.stderr.on('data', (data) => {
      const output = data.toString();
      if (output.includes('Error') || output.includes('Invalid')) {
        console.error(`FFmpeg warning for timestamp ${timeStr}:`, output.trim());
      }
    });

    ffmpegProcess.on('close', (code) => {
      if (code === 0 && frameBuffer.length > 1000) {
        resolve(frameBuffer);
      } else {
        reject(new Error(`Failed to extract frame at ${timeStr}, buffer size: ${frameBuffer.length}`));
      }
    });

    ffmpegProcess.on('error', (error) => {
      reject(new Error(`FFmpeg process error: ${error.message}`));
    });

    // Timeout for single frame extraction (extended for quality)
    setTimeout(() => {
      ffmpegProcess.kill('SIGTERM');
      if (frameBuffer.length > 1000) {
        resolve(frameBuffer);
      } else {
        reject(new Error(`Frame extraction timeout at ${timeStr}`));
      }
    }, 60000); // 60 seconds - allow plenty of time for quality extraction
  });
}

/**
 * Extract frames from video stream using FFmpeg (legacy - now using local file approach)
 * Strategic timing: early frames (ingredients), middle frames (cooking), late frames (plating)
 */
export async function extractFramesFromStream(streamUrl: string, maxFrames: number): Promise<Buffer[]> {
  return new Promise((resolve, reject) => {
    const frames: Buffer[] = [];
    
    // Calculate strategic frame times: beginning, middle sections, end
    const frameTimestamps = generateCookingTimestamps(maxFrames);
    
    const ffmpegProcess = spawn('ffmpeg', [
      '-i', streamUrl,
      '-vf', `select='${frameTimestamps.map((t, i) => `gte(t,${t})`).join('+')}',fps=1`,
      '-frames:v', maxFrames.toString(),
      '-f', 'image2pipe',
      '-vcodec', 'png',
      '-y',
      'pipe:1'
    ]);

    let currentBuffer = Buffer.alloc(0);
    const pngHeader = Buffer.from([0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A]);

    ffmpegProcess.stdout.on('data', (chunk) => {
      currentBuffer = Buffer.concat([currentBuffer, chunk]);
      
      // Parse PNG frames from buffer
      let startIndex = 0;
      while (true) {
        const headerIndex = currentBuffer.indexOf(pngHeader, startIndex);
        if (headerIndex === -1) break;
        
        if (headerIndex > startIndex) {
          const frameBuffer = currentBuffer.slice(startIndex, headerIndex);
          if (frameBuffer.length > 1000) { // Valid frame size check
            frames.push(frameBuffer);
          }
        }
        startIndex = headerIndex;
        
        const nextHeaderIndex = currentBuffer.indexOf(pngHeader, startIndex + pngHeader.length);
        if (nextHeaderIndex === -1) {
          currentBuffer = currentBuffer.slice(startIndex);
          break;
        }
      }
    });

    ffmpegProcess.stderr.on('data', (data) => {
      const output = data.toString();
      if (output.includes('Error')) {
        console.error('FFmpeg error:', output);
      }
    });

    ffmpegProcess.on('close', (code) => {
      // Handle final frame in buffer
      if (currentBuffer.length > 1000) {
        const pngHeaderIndex = currentBuffer.indexOf(pngHeader);
        if (pngHeaderIndex === 0) {
          frames.push(currentBuffer);
        }
      }
      
      if (frames.length > 0) {
        resolve(frames);
      } else {
        reject(new Error(`FFmpeg process exited with code ${code}, no valid frames extracted`));
      }
    });

    ffmpegProcess.on('error', (error) => {
      reject(new Error(`Failed to spawn FFmpeg: ${error.message}`));
    });

    // Timeout after 5 minutes - allow plenty of time for quality frame extraction
    setTimeout(() => {
      ffmpegProcess.kill('SIGTERM');
      if (frames.length > 0) {
        resolve(frames);
      } else {
        reject(new Error('Frame extraction timeout - no frames captured'));
      }
    }, 300000); // 5 minutes
  });
}

/**
 * Get video duration using FFprobe
 */
async function getVideoDuration(videoPath: string): Promise<number> {
  return new Promise((resolve, reject) => {
    const ffprobe = spawn('ffprobe', [
      '-v', 'error',
      '-show_entries', 'format=duration',
      '-of', 'csv=p=0',
      videoPath
    ]);

    let output = '';
    let errorOutput = '';

    ffprobe.stdout.on('data', (data) => {
      output += data.toString();
    });

    ffprobe.stderr.on('data', (data) => {
      errorOutput += data.toString();
    });

    ffprobe.on('close', (code) => {
      if (code === 0 && output.trim()) {
        const duration = parseFloat(output.trim());
        resolve(Math.floor(duration)); // Return duration in seconds
      } else {
        resolve(60); // Default to 60 seconds if we can't get duration
      }
    });

    ffprobe.on('error', (error) => {
      resolve(60); // Default to 60 seconds
    });
  });
}

/**
 * Generate adaptive timestamps based on video duration
 * ‚â§1 minute: Every 2 seconds for comprehensive coverage (up to ~30 frames)
 * 1-3 minutes: Enhanced timing with 7s start, 5s middle, 7s end
 * 3-5 minutes: Every 10 seconds with enhanced start/end coverage
 */
function generateAdaptiveTimestamps(durationSeconds: number): number[] {
  const timestamps: number[] = [];
  
  if (durationSeconds <= 60) {
    // ‚â§1 minute: Every 2 seconds for comprehensive coverage (up to ~30 frames)
    for (let t = 1; t < durationSeconds; t += 2) {
      timestamps.push(t);
    }
  } else if (durationSeconds <= 180) {
    // 1-3 minutes: Enhanced timing with 7s start, 5s middle, 7s end
    
    // Start section (first 30s): every 7 seconds
    for (let t = 3; t <= 30 && t < durationSeconds; t += 7) {
      timestamps.push(t);
    }
    
    // Middle section (bulk): every 5 seconds
    for (let t = 35; t < durationSeconds - 30; t += 5) {
      timestamps.push(t);
    }
    
    // End section (last 30s): every 7 seconds
    const endStart = Math.max(durationSeconds - 30, 35);
    for (let t = endStart; t < durationSeconds - 5; t += 7) {
      if (!timestamps.includes(t)) { // Avoid duplicates
        timestamps.push(t);
      }
    }
  } else if (durationSeconds <= 300) {
    // 3-5 minutes: Every 10 seconds with enhanced start/end coverage
    
    // Early section (first 30s): every 5 seconds
    for (let t = 2; t <= 30 && t < durationSeconds; t += 5) {
      timestamps.push(t);
    }
    
    // Middle section (bulk): every 10 seconds
    for (let t = 40; t < durationSeconds - 30; t += 10) {
      timestamps.push(t);
    }
    
    // Final section (last 30s): every 5 seconds
    const finalStart = Math.max(durationSeconds - 30, 40);
    for (let t = finalStart; t < durationSeconds - 5; t += 5) {
      if (!timestamps.includes(t)) { // Avoid duplicates
        timestamps.push(t);
      }
    }
  } else {
    // 5+ minutes: Every 15 seconds (up to ~20 frames, max cooking video length)
    for (let t = 5; t < Math.min(durationSeconds, 300); t += 15) {
      timestamps.push(t);
    }
  }
  
  // Ensure we don't exceed video length and sort
  const validTimestamps = timestamps
    .filter(t => t < durationSeconds - 2) // Leave 2s buffer at end
    .sort((a, b) => a - b);
    
  return validTimestamps;
}

/**
 * Legacy function - now replaced by adaptive timestamps
 */
function generateCookingTimestamps(numFrames: number): number[] {
  const timestamps: number[] = [];
  
  
  if (numFrames <= 3) {
    // For few frames: beginning, middle, end
    timestamps.push(2, 15, 30); // 2s, 15s, 30s
  } else if (numFrames <= 6) {
    // For moderate frames: spread across cooking stages
    timestamps.push(1, 5, 10, 20, 35, 45); // Strategic cooking moments
  } else {
    // For comprehensive analysis: cover full cooking journey
    timestamps.push(1, 3, 7, 12, 18, 25, 35, 50); // Detailed coverage
  }
  
  const finalTimestamps = timestamps.slice(0, numFrames);
  return finalTimestamps;
}

/**
 * Analyze video frames using OpenAI Vision API for cooking content
 * Identifies ingredients, cooking actions, techniques, and instructions
 */
export async function analyzeFramesWithVision(frames: Buffer[]): Promise<string[]> {
  const results: string[] = [];
  
  console.log(`üß† Starting AI vision analysis for ${frames.length} frames...`);
  
  // Process frames in batches to avoid rate limiting
  const BATCH_SIZE = 3; // Analyze 3 frames at a time for better rate limiting
  const BATCH_DELAY = 1000; // 1 second delay between batches
  const RETRY_DELAY = 3000; // 3 second delay for rate limit retries (more conservative)
  const MAX_RETRIES = 5; // More retry attempts for reliability
  
  console.log(`üìä Processing strategy: ${BATCH_SIZE} frames per batch, ${BATCH_DELAY}ms delay between batches`);
  
  for (let batchStart = 0; batchStart < frames.length; batchStart += BATCH_SIZE) {
    const batchEnd = Math.min(batchStart + BATCH_SIZE, frames.length);
    const batchFrames = frames.slice(batchStart, batchEnd);
    
    const batchNumber = Math.floor(batchStart / BATCH_SIZE) + 1;
    const totalBatches = Math.ceil(frames.length / BATCH_SIZE);
    console.log(`üéØ Starting batch ${batchNumber}/${totalBatches} (frames ${batchStart + 1}-${batchEnd})`);
    
    // Process frames in current batch with small delays
    for (let i = 0; i < batchFrames.length; i++) {
      const globalIndex = batchStart + i;
      let retryCount = 0;
      let analysis = '';
      
      console.log(`üîç Analyzing frame ${globalIndex + 1}/${frames.length}...`);
      
      // Retry loop for rate limiting
      while (retryCount <= MAX_RETRIES) {
        try {
          if (retryCount === 0) {
            console.log(`üì§ Sending frame ${globalIndex + 1} to OpenAI Vision API...`);
          } else {
            console.log(`üîÑ Retry attempt ${retryCount} for frame ${globalIndex + 1}...`);
          }
          
          // Convert frame to base64 for Vision API
          const base64Frame = batchFrames[i].toString('base64');
          console.log(`üìè Frame ${globalIndex + 1} encoded: ${(base64Frame.length / 1024).toFixed(1)}KB base64`);
          
          // Analyze frame with OpenAI Vision (now focused on observations only)
          analysis = await analyzeFrameWithOpenAI(base64Frame, globalIndex);
          console.log(`‚úÖ Frame ${globalIndex + 1} analysis complete (${analysis.length} characters)`);
          
          // Success - break out of retry loop
          break;
          
        } catch (error: any) {
          // Check if it's a rate limit error
          if (error?.status === 429 && retryCount < MAX_RETRIES) {
            console.log(`‚è≥ Rate limit hit for frame ${globalIndex + 1}, waiting ${RETRY_DELAY}ms before retry...`);
            await new Promise(resolve => setTimeout(resolve, RETRY_DELAY));
            retryCount++;
          } else {
            console.error(`‚ùå Failed to analyze frame ${globalIndex + 1} after ${retryCount + 1} attempts:`, error);
            break; // Exit retry loop on non-rate-limit errors or max retries reached
          }
        }
      }
      
      // Add analysis if we got one
      if (analysis.trim()) {
        results.push(analysis);
        console.log(`üìù Frame ${globalIndex + 1} analysis added to results`);
      } else {
        console.log(`‚ö†Ô∏è Frame ${globalIndex + 1} produced no usable analysis`);
      }
      
      // Small delay between individual frame analyses (only if not the last frame in batch)
      if (i < batchFrames.length - 1) {
        console.log('‚è≥ Waiting 500ms before next frame analysis...');
        await new Promise(resolve => setTimeout(resolve, 500)); // Reduced delay
      }
    }
    
    // Longer delay between batches to ensure we don't hit rate limits
    if (batchEnd < frames.length) {
      console.log(`‚è≥ Batch ${batchNumber} complete. Waiting ${BATCH_DELAY}ms before next batch...`);
      await new Promise(resolve => setTimeout(resolve, BATCH_DELAY));
    }
  }
  
  console.log(`üéâ AI vision analysis complete! ${results.length}/${frames.length} frames successfully analyzed`);
  return results;
}

/**
 * Analyze a single frame using OpenAI Vision API for cooking content
 * Now focused on OBSERVATIONS only, not recipe extraction
 */
async function analyzeFrameWithOpenAI(base64Frame: string, frameIndex: number): Promise<string> {
  const OpenAI = require('openai');
  const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY || 'missing',
  });

  if (!process.env.OPENAI_API_KEY) {
    throw new Error('OPENAI_API_KEY environment variable is required for video analysis');
  }

  const frameStage = getFrameStage(frameIndex);
  
  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
      {
        role: "user",
        content: [
          {
            type: "text",
            text: `You are analyzing a single frame from a cooking video. This frame represents the ${frameStage} stage.

IMPORTANT: Only describe what you SEE in this frame. Do NOT try to create a complete recipe or make assumptions about other frames.

INGREDIENT IDENTIFICATION GUIDELINES:
- Be precise and accurate - only identify ingredients you can clearly see
- Look for package labels or text that can help identify specific ingredients
- If unsure about specific ingredients, describe what you see rather than guessing
- Pay attention to size, color, and shape to distinguish between similar ingredients
- Note any visible brands or product names
- Do NOT assume ingredients that aren't clearly visible

Describe this frame using this format:

OBSERVATIONS:
- What ingredients are visible? (list with estimated quantities if clear, be specific and accurate)
- What cooking action is happening? (chopping, searing, mixing, etc.)
- What tools/equipment are being used?
- What's the state of the food? (raw, cooking, cooked, plated)
- Any text overlays, package labels, or measurements visible?

Keep your response concise and factual. Focus only on what is clearly visible in THIS specific frame.

If no cooking content is visible, simply respond: "No cooking content visible."`,
          },
          {
            type: "image_url",
            image_url: {
              url: `data:image/png;base64,${base64Frame}`,
            },
          },
        ],
      },
    ],
    max_tokens: 300, // Reduced since we're just observing, not extracting recipes
    temperature: 0.2, // Even lower temperature for consistent observations
  });

  return response.choices[0].message.content || '';
}

/**
 * Determine what stage of cooking this frame likely represents
 */
function getFrameStage(frameIndex: number): string {
  if (frameIndex === 0) return 'ingredient preparation';
  if (frameIndex <= 2) return 'early cooking';
  if (frameIndex >= 4) return 'final dish/plating';
  return 'mid cooking';
}

/**
 * Combine vision analysis results into coherent recipe text using AI consolidation
 * Uses ChatGPT to create one cohesive story from multiple frame analyses
 */
async function combineVisionAnalysis(analysisResults: string[]): Promise<string> {
  console.log(`üîó Combining vision analysis from ${analysisResults.length} frame analyses...`);
  
  if (analysisResults.length === 0) {
    console.log('‚ùå No analysis results to combine');
    return 'No cooking content detected in video frames.';
  }

  // Filter out empty or non-cooking content
  const validAnalyses = analysisResults.filter(result => 
    result.trim() && 
    !result.toLowerCase().includes('no cooking content') &&
    !result.toLowerCase().includes('not visible') &&
    result.length > 10
  );

  console.log(`üìä Filtering results: ${validAnalyses.length}/${analysisResults.length} frames contain valid cooking content`);

  if (validAnalyses.length === 0) {
    console.log('‚ùå No valid cooking content found in any frames');
    return 'Video does not appear to contain clear cooking instructions or ingredient information.';
  }

  // Use AI to consolidate multiple frame analyses into one cohesive recipe narrative
  try {
    console.log(`ü§ñ Using AI to consolidate ${validAnalyses.length} frame analyses into cohesive recipe...`);
    const consolidatedRecipe = await consolidateFrameAnalysesWithAI(validAnalyses);
    console.log(`‚úÖ AI consolidation complete (${consolidatedRecipe.length} characters)`);
    return consolidatedRecipe;
  } catch (error) {
    console.error('‚ùå AI consolidation failed, falling back to simple concatenation:', error);
    
    // Fallback to simple consolidation if AI fails
    const simpleNarrative = `COOKING PROCESS ANALYSIS:

${validAnalyses.map((analysis, index) => 
  `Step ${index + 1}:
${analysis}`
).join('\n\n')}`;

    console.log(`‚úÖ Fallback consolidation complete (${simpleNarrative.length} characters)`);
    return simpleNarrative;
  }
}

/**
 * Use ChatGPT to consolidate multiple frame analyses into one cohesive recipe
 */
async function consolidateFrameAnalysesWithAI(frameAnalyses: string[]): Promise<string> {
  const OpenAI = require('openai');
  const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY || 'missing',
  });

  if (!process.env.OPENAI_API_KEY) {
    throw new Error('OPENAI_API_KEY environment variable is required for analysis consolidation');
  }

  const frameData = frameAnalyses.map((analysis, index) => 
    `--- FRAME ${index + 1} ---\n${analysis}`
  ).join('\n\n');

  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
      {
        role: "user",
        content: `You are analyzing multiple frames from a cooking video to create one cohesive recipe story. Each frame shows different moments in the cooking process.

IMPORTANT GOALS:
1. Identify the SPECIFIC DISH being made (be descriptive - "Lobster Pasta" not just "Pasta")
2. Consolidate duplicate ingredients (if lobster appears in multiple frames, list it once)
3. Create a logical flow of cooking steps
4. Remove redundant observations
5. Focus on the actual cooking process and ingredients

Here are the individual frame analyses:

${frameData}

Please consolidate these observations into a coherent cooking narrative that follows this format:

DISH: [Specific name of the dish being prepared - be descriptive, include main protein/ingredients]

INGREDIENTS OBSERVED:
- [List each unique ingredient only once, with estimated quantities when visible]
- [Be specific: "lobster meat" not just "seafood", "penne pasta" not just "pasta"]

COOKING PROCESS:
1. [First step observed]
2. [Second step observed] 
3. [Continue with logical cooking progression]
4. [Final plating/presentation]

Focus on creating one unified story from these multiple observations. Eliminate duplicates and create a logical cooking flow.`
      }
    ],
    max_tokens: 800,
    temperature: 0.3,
  });

  return response.choices[0].message.content || '';
}

/**
 * Legacy OCR function - kept as fallback for text overlays
 * Use this when frames contain visible text (measurements, instructions, etc.)
 */
export async function extractTextWithOCR(frames: Buffer[]): Promise<string> {
  // Implementation kept for text overlay detection
  // This would run Tesseract OCR on frames that might contain text
  return 'OCR analysis not implemented yet - use vision analysis instead';
}
